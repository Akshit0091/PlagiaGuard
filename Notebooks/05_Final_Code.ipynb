{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5269f34d-6949-4588-827e-e48f3ec8d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-search-results) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd722de-105e-4e24-a219-5f97f6df68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "‚úÖ SerpAPI library found!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    from serpapi import GoogleSearch\n",
    "    print(\"‚úÖ All libraries imported successfully!\")\n",
    "    print(\"‚úÖ SerpAPI library found!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå SerpAPI not installed!\")\n",
    "    print(\"\\nüì¶ Run this command:\")\n",
    "    print(\"   pip install google-search-results\")\n",
    "    print(\"\\nThen restart the kernel and run again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1005bce-0be8-47d0-b1d0-c80002f2543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  WARNING: You need to add your SerpAPI key!\n",
      "\n",
      "üìã Quick Setup:\n",
      "   1. Go to: https://serpapi.com/\n",
      "   2. Sign up (free)\n",
      "   3. Copy your API key from dashboard\n",
      "   4. Paste it above (replace YOUR_API_KEY_HERE)\n",
      "\n",
      "‚úÖ Free tier gives you 100 searches/month\n"
     ]
    }
   ],
   "source": [
    "SERPAPI_KEY = \"5317f506f5ae86fd7689c0db2cec4cce590a26e849c125b932114cb1292d3944\"\n",
    "\n",
    "if SERPAPI_KEY == \"5317f506f5ae86fd7689c0db2cec4cce590a26e849c125b932114cb1292d3944\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: You need to add your SerpAPI key!\")\n",
    "    print(\"\\nüìã Quick Setup:\")\n",
    "    print(\"   1. Go to: https://serpapi.com/\")\n",
    "    print(\"   2. Sign up (free)\")\n",
    "    print(\"   3. Copy your API key from dashboard\")\n",
    "    print(\"   4. Paste it above (replace YOUR_API_KEY_HERE)\")\n",
    "    print(\"\\n‚úÖ Free tier gives you 100 searches/month\")\n",
    "else:\n",
    "    print(f\"‚úÖ API Key configured: {SERPAPI_KEY[:10]}...{SERPAPI_KEY[-4:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c407cf-510c-4fe2-af39-1a02ed0fdd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PhraseExtractor initialized\n"
     ]
    }
   ],
   "source": [
    "class PhraseExtractor:\n",
    "    \"\"\"\n",
    "    Extract key phrases from text for searching\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_sentences(self, text, min_words=8, max_words=15):\n",
    "        \"\"\"\n",
    "        Extract meaningful sentences for searching\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        key_phrases = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.strip().split()\n",
    "            if min_words <= len(words) <= max_words:\n",
    "                clean = ' '.join(words)\n",
    "                key_phrases.append(clean)\n",
    "        \n",
    "        return key_phrases[:5]\n",
    "    \n",
    "    def extract_ngrams(self, text, n=4):\n",
    "        \"\"\"\n",
    "        Extract n-grams (sequences of n words)\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        ngrams = []\n",
    "        \n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = ' '.join(words[i:i+n])\n",
    "            if len(ngram) > 20:\n",
    "                ngrams.append(ngram)\n",
    "        \n",
    "        return ngrams[:10]\n",
    "    \n",
    "    def get_search_queries(self, text):\n",
    "        \"\"\"\n",
    "        Generate search queries from text\n",
    "        \"\"\"\n",
    "        queries = []\n",
    "        \n",
    "        sentences = self.extract_sentences(text)\n",
    "        queries.extend(sentences[:3])\n",
    "        \n",
    "        ngrams = self.extract_ngrams(text)\n",
    "        queries.extend(ngrams[:2])\n",
    "        \n",
    "        return queries\n",
    "\n",
    "extractor = PhraseExtractor()\n",
    "print(\"‚úÖ PhraseExtractor initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f5ea33-36c7-41c0-ae93-e5d016456407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SerpAPI Searcher initialized\n",
      "‚úÖ API Key is valid!\n"
     ]
    }
   ],
   "source": [
    "class SerpAPISearcher:\n",
    "    \"\"\"\n",
    "    Web scraper using SerpAPI - 100% reliable!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        print(f\"‚úÖ SerpAPI Searcher initialized\")\n",
    "        \n",
    "        \n",
    "        if api_key and api_key != \"YOUR_API_KEY_HERE\":\n",
    "            self.test_api_key()\n",
    "    \n",
    "    def test_api_key(self):\n",
    "        \"\"\"\n",
    "        Test if API key is valid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                \"q\": \"test\",\n",
    "                \"api_key\": self.api_key,\n",
    "                \"num\": 1\n",
    "            }\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            if \"error\" in results:\n",
    "                print(f\"‚ùå API Key Error: {results['error']}\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"‚úÖ API Key is valid!\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API Test Failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def search_google(self, query, num_results=5):\n",
    "        \"\"\"\n",
    "        Search Google using SerpAPI - NO BLOCKING!\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"üîç Searching: '{query[:50]}...'\")\n",
    "            \n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"api_key\": self.api_key,\n",
    "                \"num\": num_results,\n",
    "                \"engine\": \"google\"\n",
    "            }\n",
    "            \n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            \n",
    "            if \"error\" in results:\n",
    "                print(f\"   ‚ùå Error: {results['error']}\")\n",
    "                return []\n",
    "            \n",
    "            \n",
    "            urls = []\n",
    "            for result in results.get(\"organic_results\", []):\n",
    "                url = result.get(\"link\")\n",
    "                if url and url.startswith('http'):\n",
    "                    urls.append(url)\n",
    "            \n",
    "            print(f\"   ‚úÖ Found {len(urls)} URLs\")\n",
    "            return urls[:num_results]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {str(e)[:80]}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_text_from_url(self, url, timeout=10):\n",
    "        \"\"\"\n",
    "        Extract text content from webpage\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, headers=self.headers, timeout=timeout)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            \n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
    "                element.decompose()\n",
    "            \n",
    "            \n",
    "            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "            \n",
    "            if main_content:\n",
    "                text = main_content.get_text(separator=' ', strip=True)\n",
    "            else:\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "            \n",
    "            \n",
    "            text = ' '.join(text.split())\n",
    "            return text[:5000]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "\n",
    "if SERPAPI_KEY != \"YOUR_API_KEY_HERE\":\n",
    "    scraper = SerpAPISearcher(SERPAPI_KEY)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Scraper not initialized - add API key in CELL 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6159fa9a-470a-4d30-837a-779763bc95bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SerpAPI Searcher initialized\n",
      "‚úÖ API Key is valid!\n",
      "‚úÖ WebPlagiarismDetector initialized\n"
     ]
    }
   ],
   "source": [
    "class WebPlagiarismDetector:\n",
    "    \"\"\"\n",
    "    Complete web-based plagiarism detection system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.scraper = SerpAPISearcher(api_key)\n",
    "        self.extractor = PhraseExtractor()\n",
    "        print(\"‚úÖ WebPlagiarismDetector initialized\")\n",
    "    \n",
    "    def check_plagiarism(self, user_text, num_searches=2, results_per_search=3):\n",
    "        \"\"\"\n",
    "        Check text against web sources using SerpAPI\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"üåê WEB-BASED PLAGIARISM DETECTION (SerpAPI)\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n‚è±Ô∏è  Estimated time: 10-20 seconds\")\n",
    "        \n",
    "        print(\"\\nüîç Step 1: Extracting key phrases...\")\n",
    "        queries = self.extractor.get_search_queries(user_text)[:num_searches]\n",
    "        \n",
    "        if not queries:\n",
    "            print(\"‚ùå Could not extract phrases\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(queries)} search queries:\")\n",
    "        for i, q in enumerate(queries, 1):\n",
    "            preview = q[:60] + \"...\" if len(q) > 60 else q\n",
    "            print(f\"   {i}. {preview}\")\n",
    "        \n",
    "        print(f\"\\nüîç Step 2: Searching the web via SerpAPI...\")\n",
    "        all_urls = []\n",
    "        \n",
    "        for i, query in enumerate(queries, 1):\n",
    "            print(f\"\\n   Search {i}/{len(queries)}:\")\n",
    "            urls = self.scraper.search_google(query, num_results=results_per_search)\n",
    "            all_urls.extend(urls)\n",
    "            \n",
    "            if i < len(queries):\n",
    "                print(f\"   ‚è≥ Waiting 2 seconds...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        all_urls = list(set(all_urls))[:10]\n",
    "        print(f\"\\n‚úÖ Total unique sources: {len(all_urls)}\")\n",
    "        \n",
    "        if len(all_urls) == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  No URLs found\")\n",
    "            print(\"   Check your API key and internet connection\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüìÑ Step 3: Fetching content from sources...\")\n",
    "        sources = []\n",
    "        \n",
    "        for i, url in enumerate(all_urls, 1):\n",
    "            domain = urlparse(url).netloc\n",
    "            print(f\"   {i}/{len(all_urls)} {domain[:45]:<45}\", end=\" \")\n",
    "            \n",
    "            content = self.scraper.extract_text_from_url(url)\n",
    "            \n",
    "            if content and len(content) > 100:\n",
    "                sources.append({\n",
    "                    'url': url,\n",
    "                    'content': content,\n",
    "                    'domain': domain\n",
    "                })\n",
    "                print(\"‚úÖ\")\n",
    "            else:\n",
    "                print(\"‚ùå\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully fetched {len(sources)} sources\")\n",
    "        \n",
    "        if len(sources) == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  Could not fetch any content\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüìä Step 4: Calculating similarity...\")\n",
    "        \n",
    "        results = []\n",
    "        user_words = set(user_text.lower().split())\n",
    "        \n",
    "        for source in sources:\n",
    "            source_words = set(source['content'].lower().split())\n",
    "            common = user_words.intersection(source_words)\n",
    "            \n",
    "            if len(user_words) > 0:\n",
    "                similarity = (len(common) / len(user_words)) * 100\n",
    "                \n",
    "                results.append({\n",
    "                    'url': source['url'],\n",
    "                    'domain': source['domain'],\n",
    "                    'similarity': similarity,\n",
    "                    'common_words': len(common)\n",
    "                })\n",
    "        \n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "        print(f\"‚úÖ Calculation complete!\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, results):\n",
    "        \"\"\"\n",
    "        Generate plagiarism report with visualization\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìã PLAGIARISM DETECTION REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"\\n‚úÖ No similar content found!\")\n",
    "            print(\"   Your text appears to be original.\")\n",
    "            return None\n",
    "        \n",
    "        max_similarity = max(r['similarity'] for r in results)\n",
    "        \n",
    "        if max_similarity >= 60:\n",
    "            verdict = \"üî¥ HIGH RISK\"\n",
    "        elif max_similarity >= 40:\n",
    "            verdict = \"üü† MODERATE RISK\"\n",
    "        elif max_similarity >= 20:\n",
    "            verdict = \"üü° LOW RISK\"\n",
    "        else:\n",
    "            verdict = \"üü¢ MINIMAL RISK\"\n",
    "        \n",
    "        print(f\"\\n{verdict}\")\n",
    "        print(f\"üìä Highest Similarity: {max_similarity:.1f}%\")\n",
    "        print(f\"üîç Sources Checked: {len(results)}\")\n",
    "        \n",
    "        print(f\"\\nüîó Top {min(5, len(results))} Matching Sources:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for i, result in enumerate(results[:5], 1):\n",
    "            print(f\"\\n{i}. {result['domain']}\")\n",
    "            print(f\"   Similarity: {result['similarity']:.1f}%\")\n",
    "            print(f\"   Common words: {result['common_words']}\")\n",
    "            print(f\"   URL: {result['url'][:65]}...\")\n",
    "        \n",
    "        # Create visualization\n",
    "        if len(results) > 0:\n",
    "            df = pd.DataFrame(results[:10])\n",
    "            \n",
    "            plt.figure(figsize=(12, 7))\n",
    "            \n",
    "            top_results = results[:min(8, len(results))]\n",
    "            domains = [r['domain'][:35] for r in top_results]\n",
    "            similarities = [r['similarity'] for r in top_results]\n",
    "            \n",
    "            colors = ['red' if s >= 60 else 'orange' if s >= 40 else 'yellow' if s >= 20 else 'green' \n",
    "                      for s in similarities]\n",
    "            \n",
    "            plt.barh(domains, similarities, color=colors, edgecolor='black', linewidth=1.2)\n",
    "            plt.xlabel('Similarity Score (%)', fontsize=12, fontweight='bold')\n",
    "            plt.title('Web-Based Plagiarism Detection Results (SerpAPI)', fontsize=14, fontweight='bold')\n",
    "            plt.xlim(0, 100)\n",
    "            \n",
    "            plt.axvline(x=60, color='red', linestyle='--', alpha=0.4, linewidth=2, label='High (60%)')\n",
    "            plt.axvline(x=40, color='orange', linestyle='--', alpha=0.4, linewidth=2, label='Moderate (40%)')\n",
    "            plt.axvline(x=20, color='yellow', linestyle='--', alpha=0.4, linewidth=2, label='Low (20%)')\n",
    "            \n",
    "            plt.legend(loc='lower right', fontsize=10)\n",
    "            plt.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        return None\n",
    "\n",
    "if SERPAPI_KEY != \"YOUR_API_KEY_HERE\":\n",
    "    detector = WebPlagiarismDetector(SERPAPI_KEY)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Detector not initialized - add API key in CELL 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2642e332-2e74-4338-a9f9-40512a55166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING WEB PLAGIARISM CHECKER\n",
      "======================================================================\n",
      "\n",
      "Test Text (2 words):\n",
      "\n",
      "Your Text\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Starting check...\n",
      "\n",
      "======================================================================\n",
      "üåê WEB-BASED PLAGIARISM DETECTION (SerpAPI)\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Estimated time: 10-20 seconds\n",
      "\n",
      "üîç Step 1: Extracting key phrases...\n",
      "‚ùå Could not extract phrases\n",
      "\n",
      "‚ö†Ô∏è  No results obtained\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Your Text\n",
    "\"\"\"\n",
    "\n",
    "if SERPAPI_KEY != \"YOUR_API_KEY_HERE\":\n",
    "    print(\"üß™ TESTING WEB PLAGIARISM CHECKER\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTest Text ({len(sample_text.split())} words):\")\n",
    "    print(sample_text)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚è≥ Starting check...\\n\")\n",
    "    \n",
    "    results = detector.check_plagiarism(\n",
    "        sample_text,\n",
    "        num_searches=2,\n",
    "        results_per_search=3\n",
    "    )\n",
    "    \n",
    "    if results and len(results) > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üéâ CHECK SUCCESSFUL!\")\n",
    "        print(\"=\"*70)\n",
    "        report_df = detector.generate_report(results)\n",
    "        \n",
    "        print(\"\\nüìä Summary Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Total sources: {len(results)}\")\n",
    "        print(f\"   ‚Ä¢ Highest similarity: {max(r['similarity'] for r in results):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Average similarity: {sum(r['similarity'] for r in results)/len(results):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Sources above 40%: {sum(1 for r in results if r['similarity'] >= 40)}\")\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if report_df is not None and not report_df.empty:\n",
    "            output_file = f\"plagiarism_results_{timestamp}.csv\"\n",
    "            report_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\nüíæ Results saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No results obtained\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please add your SerpAPI key in CELL 2 first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94373e0-8226-4107-ba90-055b81d6e3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
